{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The work in this notebook has been referenced from the work done by my professor Mrs.Swati Jain https://scholar.google.com/citations?user=aU8LyHYAAAAJ&hl=en"
      ],
      "metadata": {
        "id": "Tz6iAVQyXVf0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2024-11-13T17:50:15.817563Z",
          "iopub.status.busy": "2024-11-13T17:50:15.817134Z",
          "iopub.status.idle": "2024-11-13T17:50:15.824082Z",
          "shell.execute_reply": "2024-11-13T17:50:15.823000Z",
          "shell.execute_reply.started": "2024-11-13T17:50:15.817522Z"
        },
        "trusted": true,
        "id": "Zh0ED_Dy65LD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-13T17:50:41.670808Z",
          "iopub.status.busy": "2024-11-13T17:50:41.670338Z",
          "iopub.status.idle": "2024-11-13T17:51:01.715857Z",
          "shell.execute_reply": "2024-11-13T17:51:01.714699Z",
          "shell.execute_reply.started": "2024-11-13T17:50:41.670763Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xG5Q4sBp65LN",
        "outputId": "a206f63a-3547-4a5d-d58c-772362003f52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "# Download NLTK data for tokenization\n",
        "nltk.download('punkt')\n",
        "if torch.cuda.is_available():\n",
        "    device=torch.device(type=\"cuda\",index=0)\n",
        "else:\n",
        "    device=torch.device(type=\"cpu\",index=0)\n",
        "# Parameters\n",
        "image_size = 224\n",
        "embedding_dim = 256\n",
        "hidden_dim = 512\n",
        "freq_threshold = 5\n",
        "batch_size = 32\n",
        "num_epochs = 1\n",
        "learning_rate = 0.001\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-13T17:52:25.881527Z",
          "iopub.status.busy": "2024-11-13T17:52:25.881093Z",
          "iopub.status.idle": "2024-11-13T17:52:25.892357Z",
          "shell.execute_reply": "2024-11-13T17:52:25.891063Z",
          "shell.execute_reply.started": "2024-11-13T17:52:25.881485Z"
        },
        "trusted": true,
        "id": "dONhGgNW65LR"
      },
      "outputs": [],
      "source": [
        "# Vocabulary class to build word-to-index and index-to-word mappings\n",
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold):\n",
        "        self.freq_threshold = freq_threshold\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.stoi = {v: k for k, v in self.itos.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        frequencies = Counter()\n",
        "        idx = 4  # Starting index for new words\n",
        "\n",
        "        for sentence in sentence_list:\n",
        "            tokens = word_tokenize(sentence.lower())\n",
        "            frequencies.update(tokens)\n",
        "\n",
        "            for token, freq in frequencies.items():\n",
        "                if freq >= self.freq_threshold and token not in self.stoi:\n",
        "                    self.stoi[token] = idx\n",
        "                    self.itos[idx] = token\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        return [self.stoi.get(token, self.stoi[\"<UNK>\"]) for token in tokens]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the vocabulary\n",
        "\n",
        "captions_file = '/kaggle/input/flickr8k/captions.txt'\n",
        "img_folder = '/kaggle/input/flickr8k/Images'\n",
        "captions_list = []\n",
        "\n",
        "# Read all captions and build vocabulary\n",
        "with open(captions_file, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "    for line in lines:\n",
        "        image, caption = line.strip().split(',')[0], line.strip().split(',')[1]\n",
        "        captions_list.append(caption)\n",
        "\n",
        "vocab = Vocabulary(freq_threshold)\n",
        "vocab.build_vocabulary(captions_list)\n",
        "\n",
        "# Dataset class for Flickr8k\n"
      ],
      "metadata": {
        "id": "52MFgzYmWbJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-13T18:33:32.567721Z",
          "iopub.status.busy": "2024-11-13T18:33:32.567279Z",
          "iopub.status.idle": "2024-11-13T18:33:32.580443Z",
          "shell.execute_reply": "2024-11-13T18:33:32.579152Z",
          "shell.execute_reply.started": "2024-11-13T18:33:32.567658Z"
        },
        "trusted": true,
        "id": "gQeUUsiA65LU"
      },
      "outputs": [],
      "source": [
        "class Flickr8kDataset(Dataset):\n",
        "    def __init__(self, img_folder, captions_file, transform=None, vocab=None):\n",
        "        self.img_folder = img_folder\n",
        "        self.transform = transform\n",
        "        self.vocab = vocab\n",
        "        self.captions = self.load_captions(captions_file)\n",
        "\n",
        "    def load_captions(self, captions_file):\n",
        "        with open(captions_file, 'r') as file:\n",
        "            lines = file.readlines()\n",
        "        captions = {}\n",
        "        for line in lines:\n",
        "            img, caption = line.strip().split(',')[0], line.strip().split(',')[1]\n",
        "            img_id = img.split('#')[0]\n",
        "            if img_id not in captions:\n",
        "                captions[img_id] = []\n",
        "            captions[img_id].append(caption)\n",
        "        return captions\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.captions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = list(self.captions.keys())[idx]\n",
        "        img_path = os.path.join(self.img_folder, img_id)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        caption = self.captions[img_id][0]\n",
        "        caption = [self.vocab.stoi[\"<SOS>\"]] + self.vocab.numericalize(caption) + [self.vocab.stoi[\"<EOS>\"]]\n",
        "        return image, caption  # Return caption as a list, not a tensor\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-13T18:33:35.349202Z",
          "iopub.status.busy": "2024-11-13T18:33:35.348768Z",
          "iopub.status.idle": "2024-11-13T18:33:35.368952Z",
          "shell.execute_reply": "2024-11-13T18:33:35.367473Z",
          "shell.execute_reply.started": "2024-11-13T18:33:35.349161Z"
        },
        "trusted": true,
        "id": "K4mPZpJf65LW"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images = []\n",
        "    captions = []\n",
        "\n",
        "    for img, caption in batch:\n",
        "        images.append(img)\n",
        "        captions.append(torch.tensor(caption, dtype=torch.long))\n",
        "\n",
        "    # Stack images and pad captions\n",
        "    images = torch.stack(images)\n",
        "    captions = pad_sequence(captions, batch_first=True, padding_value=vocab.stoi[\"<PAD>\"])\n",
        "    print('images number', images.shape)\n",
        "    print('Caption number', captions.shape)\n",
        "\n",
        "    return images, captions\n",
        "\n",
        "# Encoder model\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # Load pretrained ResNet50 model with weights\n",
        "        weights_path = '/kaggle/input/resnet50/pytorch/default/1/resnet50-0676ba61.pth'\n",
        "        resnet = resnet50(weights=None)\n",
        "        resnet.load_state_dict(torch.load(weights_path))\n",
        "\n",
        "        # Remove the last fully connected layer\n",
        "        self.resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
        "        self.fc = nn.Linear(resnet.fc.in_features, embed_size)\n",
        "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.resnet(images)\n",
        "        features = features.view(features.size(0), -1)\n",
        "        features = self.bn(self.fc(features))\n",
        "        return features\n",
        "\n",
        "# Decoder model\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        embeddings = self.embed(captions)\n",
        "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
        "        hiddens, _ = self.lstm(embeddings)\n",
        "        outputs = self.linear(hiddens)\n",
        "        return outputs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-13T18:33:38.780731Z",
          "iopub.status.busy": "2024-11-13T18:33:38.780255Z",
          "iopub.status.idle": "2024-11-13T18:33:39.640142Z",
          "shell.execute_reply": "2024-11-13T18:33:39.638922Z",
          "shell.execute_reply.started": "2024-11-13T18:33:38.780685Z"
        },
        "trusted": true,
        "id": "r3nRmwIY65LX",
        "outputId": "8784894b-0485-406e-f3b9-61a8c20b443d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_31/2438455425.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  resnet.load_state_dict(torch.load(weights_path))\n"
          ]
        }
      ],
      "source": [
        "# Initialize the dataset and dataloader\n",
        "dataset = Flickr8kDataset(img_folder, captions_file, transform=transform, vocab=vocab)\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "# Initialize models, loss, and optimizer\n",
        "encoder = Encoder(embed_size=embedding_dim).to(device)\n",
        "decoder = Decoder(embed_size=embedding_dim, hidden_size=hidden_dim, vocab_size=len(vocab)).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<PAD>\"])\n",
        "params = list(decoder.parameters()) + list(encoder.fc.parameters()) + list(encoder.bn.parameters())\n",
        "optimizer = optim.Adam(params, lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-13T18:33:42.952461Z",
          "iopub.status.busy": "2024-11-13T18:33:42.952039Z",
          "iopub.status.idle": "2024-11-13T19:12:48.658257Z",
          "shell.execute_reply": "2024-11-13T19:12:48.656664Z",
          "shell.execute_reply.started": "2024-11-13T18:33:42.952422Z"
        },
        "trusted": true,
        "id": "zzglpvcI65LZ"
      },
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for i, (images, captions) in enumerate(data_loader):\n",
        "        images, captions = images.to(device), captions.to(device)\n",
        "\n",
        "        # Forward pass through encoder\n",
        "        features = encoder(images)\n",
        "\n",
        "        # Pass all tokens except the last one to the decoder\n",
        "        outputs = decoder(features, captions[:, :-1])  # Predict the next token for each token in captions\n",
        "\n",
        "\n",
        "        outputs = outputs.view(-1, outputs.shape[2])  # Flatten to (batch_size * (seq_len - 1), vocab_size)\n",
        "        targets = captions[:, :].contiguous().view(-1)  # Flatten to (batch_size * (seq_len - 1))\n",
        "\n",
        "        # Debug: Print shapes to verify alignment before loss calculation\n",
        "        print(f\"Outputs shape: {outputs.shape}\")  # Should be (batch_size * (seq_len - 1), vocab_size)\n",
        "        print(f\"Targets shape: {targets.shape}\")  # Should be (batch_size * (seq_len - 1))\n",
        "\n",
        "        # Calculate loss, ignoring <PAD> tokens\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Epoch [{epoch}/{num_epochs}], Step [{i}/{len(data_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(\"Training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-13T19:29:44.178034Z",
          "iopub.status.busy": "2024-11-13T19:29:44.177542Z",
          "iopub.status.idle": "2024-11-13T19:29:44.368262Z",
          "shell.execute_reply": "2024-11-13T19:29:44.367056Z",
          "shell.execute_reply.started": "2024-11-13T19:29:44.177990Z"
        },
        "trusted": true,
        "id": "6fx4_gQ565La",
        "outputId": "683a20c0-6b45-4742-db27-b75dddb36ff1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Caption: a man in a man in a man in a man in a man in a man in a man\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "def load_image(image_path, transform=None):\n",
        "    \"\"\"Load an image and apply the necessary transforms.\"\"\"\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    if transform is not None:\n",
        "        image = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "    return image\n",
        "\n",
        "def generate_caption(encoder, decoder, image_path, vocab, max_length=20):\n",
        "    \"\"\"Generate a caption for a given image.\"\"\"\n",
        "\n",
        "    # Prepare the image\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    ])\n",
        "\n",
        "    image = load_image(image_path, transform).to(device)\n",
        "\n",
        "    # Encode the image to obtain the feature vector\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        features = encoder(image)\n",
        "\n",
        "        # Initialize the caption generation\n",
        "        caption = []\n",
        "        input_token = torch.tensor([vocab.stoi[\"<SOS>\"]]).unsqueeze(0).to(device)  # Start with the <SOS> token\n",
        "\n",
        "        # Generate words one by one\n",
        "        for _ in range(max_length):\n",
        "            embeddings = decoder.embed(input_token)\n",
        "            embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
        "            hiddens, _ = decoder.lstm(embeddings)\n",
        "\n",
        "            # Get the most recent output (last timestep)\n",
        "            output = decoder.linear(hiddens.squeeze(1)[:, -1, :])  # Shape: (1, vocab_size)\n",
        "\n",
        "            # Get the most likely next token\n",
        "            _, predicted = output.max(1)\n",
        "            predicted_token = predicted.item()\n",
        "            input_token = predicted.unsqueeze(0)  # Set predicted token as the next input\n",
        "\n",
        "            # Stop if <EOS> token is generated\n",
        "            if predicted_token == vocab.stoi[\"<EOS>\"]:\n",
        "                break\n",
        "\n",
        "            # Append predicted word to the caption list\n",
        "            caption.append(predicted_token)\n",
        "\n",
        "    # Convert token indices to words\n",
        "    caption_words = [vocab.itos[token] for token in caption]\n",
        "    return \" \".join(caption_words)\n",
        "\n",
        "# Example usage:\n",
        "image_path = \"/kaggle/input/prac9img/prac9img.jpg\"  # Replace with the path to an image\n",
        "caption = generate_caption(encoder, decoder, image_path, vocab)\n",
        "print(\"Generated Caption:\", caption)\n"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 623289,
          "sourceId": 1111676,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6080219,
          "sourceId": 9898418,
          "sourceType": "datasetVersion"
        },
        {
          "isSourceIdPinned": true,
          "modelId": 163620,
          "modelInstanceId": 141014,
          "sourceId": 165727,
          "sourceType": "modelInstanceVersion"
        }
      ],
      "dockerImageVersionId": 30786,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}