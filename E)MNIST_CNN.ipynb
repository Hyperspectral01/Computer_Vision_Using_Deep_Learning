{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8Q-zfSKeZ35",
        "outputId": "6a8af471-b757-45e0-a376-16b8bf74873a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 11399531.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 353912.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 3202735.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4307150.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Epoch No: 1\n",
            "Batch: 1 / 938 Running Loss: 2.81 Running Accuracy: 4.69\n",
            "Batch: 101 / 938 Running Loss: 0.7 Running Accuracy: 88.83\n",
            "Batch: 201 / 938 Running Loss: 0.57 Running Accuracy: 92.63\n",
            "Batch: 301 / 938 Running Loss: 0.5 Running Accuracy: 94.17\n",
            "Batch: 401 / 938 Running Loss: 0.45 Running Accuracy: 95.06\n",
            "Batch: 501 / 938 Running Loss: 0.41 Running Accuracy: 95.6\n",
            "Batch: 601 / 938 Running Loss: 0.38 Running Accuracy: 96.05\n",
            "Batch: 701 / 938 Running Loss: 0.36 Running Accuracy: 96.36\n",
            "Batch: 801 / 938 Running Loss: 0.34 Running Accuracy: 96.58\n",
            "Batch: 901 / 938 Running Loss: 0.32 Running Accuracy: 96.79\n",
            "Training: Epoch Loss: 0.31 Epoch Accuracy: 96.84\n",
            "--------------------------------------------------\n",
            "Epoch No: 2\n",
            "Batch: 1 / 938 Running Loss: 0.19 Running Accuracy: 95.31\n",
            "Batch: 101 / 938 Running Loss: 0.15 Running Accuracy: 98.78\n",
            "Batch: 201 / 938 Running Loss: 0.14 Running Accuracy: 98.68\n",
            "Batch: 301 / 938 Running Loss: 0.14 Running Accuracy: 98.74\n",
            "Batch: 401 / 938 Running Loss: 0.13 Running Accuracy: 98.84\n",
            "Batch: 501 / 938 Running Loss: 0.13 Running Accuracy: 98.84\n",
            "Batch: 601 / 938 Running Loss: 0.12 Running Accuracy: 98.82\n",
            "Batch: 701 / 938 Running Loss: 0.12 Running Accuracy: 98.81\n",
            "Batch: 801 / 938 Running Loss: 0.12 Running Accuracy: 98.82\n",
            "Batch: 901 / 938 Running Loss: 0.11 Running Accuracy: 98.84\n",
            "Training: Epoch Loss: 0.11 Epoch Accuracy: 98.83\n",
            "--------------------------------------------------\n",
            "Epoch No: 3\n",
            "Batch: 1 / 938 Running Loss: 0.12 Running Accuracy: 98.44\n",
            "Batch: 101 / 938 Running Loss: 0.08 Running Accuracy: 99.09\n",
            "Batch: 201 / 938 Running Loss: 0.08 Running Accuracy: 99.11\n",
            "Batch: 301 / 938 Running Loss: 0.08 Running Accuracy: 99.09\n",
            "Batch: 401 / 938 Running Loss: 0.07 Running Accuracy: 99.13\n",
            "Batch: 501 / 938 Running Loss: 0.08 Running Accuracy: 99.04\n",
            "Batch: 601 / 938 Running Loss: 0.07 Running Accuracy: 99.05\n",
            "Batch: 701 / 938 Running Loss: 0.07 Running Accuracy: 99.06\n",
            "Batch: 801 / 938 Running Loss: 0.07 Running Accuracy: 99.0\n",
            "Batch: 901 / 938 Running Loss: 0.07 Running Accuracy: 99.01\n",
            "Training: Epoch Loss: 0.07 Epoch Accuracy: 99.0\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device=torch.device(type=\"cuda\",index=0)\n",
        "else:\n",
        "    device=torch.device(type=\"cpu\",index=0)\n",
        "\n",
        "print(device)\n",
        "\n",
        "train_dataset = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_dataset = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "batch_size=64\n",
        "\n",
        "train_dl=DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "test_dl=DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "class DRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.relu=nn.ReLU()\n",
        "        self.conv1=nn.Conv2d(in_channels=1,out_channels=8, kernel_size=(3,3), stride=1, padding=0)\n",
        "        self.bn1=nn.BatchNorm2d(8)\n",
        "        self.mp1=nn.MaxPool2d(kernel_size=(2,2),stride=2,padding=0)\n",
        "\n",
        "        self.conv2=nn.Conv2d(in_channels=8,out_channels=16, kernel_size=(3,3), stride=1, padding=0)\n",
        "        self.bn2=nn.BatchNorm2d(16)\n",
        "\n",
        "        self.conv3=nn.Conv2d(in_channels=16,out_channels=32, kernel_size=(3,3), stride=1, padding=0)\n",
        "        self.bn3=nn.BatchNorm2d(32)\n",
        "\n",
        "        self.conv4=nn.Conv2d(in_channels=32,out_channels=64, kernel_size=(3,3), stride=1, padding=0)\n",
        "        self.bn4=nn.BatchNorm2d(64)\n",
        "\n",
        "        self.flatten=nn.Flatten()\n",
        "\n",
        "        self.lin1=nn.Linear(in_features=3136, out_features=10)\n",
        "        self.bn5=nn.BatchNorm1d(num_features=10)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=self.conv1(x)\n",
        "        x=self.bn1(x)\n",
        "        x=self.relu(x)\n",
        "        x=self.mp1(x)\n",
        "\n",
        "        x=self.conv2(x)\n",
        "        x=self.bn2(x)\n",
        "        x=self.relu(x)\n",
        "\n",
        "        x=self.conv3(x)\n",
        "        x=self.bn3(x)\n",
        "        x=self.relu(x)\n",
        "\n",
        "        x=self.conv4(x)\n",
        "        x=self.bn4(x)\n",
        "        x=self.relu(x)\n",
        "\n",
        "        x=self.flatten(x)\n",
        "\n",
        "        x=self.lin1(x)\n",
        "        output=self.bn5(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "def train_one_epoch(dataloader, model,loss_fn, optimizer):\n",
        "    model.train()\n",
        "    track_loss=0\n",
        "    num_correct=0\n",
        "    for i, (imgs, labels) in enumerate(dataloader):\n",
        "        imgs=imgs.to(device)\n",
        "        labels=labels.to(device)\n",
        "        pred=model(imgs)\n",
        "\n",
        "        loss=loss_fn(pred,labels)\n",
        "        track_loss+=loss.item()\n",
        "        num_correct+=(torch.argmax(pred,dim=1)==labels).type(torch.float).sum().item()\n",
        "\n",
        "        running_loss=round(track_loss/(i+(imgs.shape[0]/batch_size)),2)\n",
        "        running_acc=round((num_correct/((i*batch_size+imgs.shape[0])))*100,2)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if i%100==0:\n",
        "            print(\"Batch:\", i+1, \"/\",len(dataloader), \"Running Loss:\",running_loss, \"Running Accuracy:\",running_acc)\n",
        "\n",
        "    epoch_loss=running_loss\n",
        "    epoch_acc=running_acc\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def eval(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "\n",
        "    total_samples = 0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in dataloader:  # Assuming your dataloader yields (images, labels)\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)  # Move labels to the same device\n",
        "\n",
        "            pred = model(imgs)\n",
        "            pred_classes = torch.argmax(pred, dim=1).type(torch.int).cpu()\n",
        "\n",
        "            # Update the total samples and correct predictions\n",
        "            total_samples += labels.size(0)\n",
        "            correct_predictions += (pred_classes == labels.cpu()).sum().item()\n",
        "\n",
        "    accuracy = correct_predictions / total_samples if total_samples > 0 else 0\n",
        "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "model=DRNN()\n",
        "model=model.to(device)\n",
        "loss_fn=nn.CrossEntropyLoss()\n",
        "lr=0.001\n",
        "#optimizer=torch.optim.SGD(params=model.parameters(), lr=lr)\n",
        "optimizer=torch.optim.Adam(params=model.parameters(), lr=lr)\n",
        "n_epochs=3\n",
        "\n",
        "for i in range(n_epochs):\n",
        "    print(\"Epoch No:\",i+1)\n",
        "    train_epoch_loss, train_epoch_acc=train_one_epoch(train_dl,model,loss_fn,optimizer)\n",
        "    print(\"Training:\", \"Epoch Loss:\", train_epoch_loss, \"Epoch Accuracy:\", train_epoch_acc)\n",
        "    print(\"--------------------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9VZye8KdtEs",
        "outputId": "d6f04441-f6a4-4baa-d35f-e16aad49ff04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 99.21%\n"
          ]
        }
      ],
      "source": [
        "eval(test_dl, model,loss_fn)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}